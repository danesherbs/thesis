\relax 
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{iii}}
\citation{Knight2016}
\citation{Darrach}
\citation{Reingold2001}
\citation{Garnelo2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}}
\citation{Hutter2005}
\citation{Mnih2015}
\citation{Silver2016}
\citation{Rosen2012}
\citation{Rosen2012}
\citation{Ormerod2016}
\citation{Ormerod2016}
\citation{Garnelo2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces May 1997: Gary Kasparov makes his first move against IBM's Deep Blue. Deep Blue would later emerge the victor in the best of six games; the first time a reigning world chess champion is defeated by a computer. \cite  {Rosen2012}.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:deep_blue_kasparov}{{1.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces March 2016: Lee Sedol, one of the greatest modern Go players, plays his first move of game three against AlphaGo. AlphaGo won four of five games. This feat was considered by many to be a decade away. \cite  {Ormerod2016}.\relax }}{2}}
\newlabel{fig:alpha_go_first_move_game_three}{{1.2}{2}}
\citation{Garnelo2016}
\citation{Garnelo2016}
\citation{Garnelo2016}
\citation{Higgins2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Overview of deep symbolic reinforcement learning system architecture. \textbf  {A}: The neural back end maps high-dimensional raw input data to a compositionally structured symbolic representation. \textbf  {B}: The compositionally structured symbolic representation. \textbf  {C}: Reinforcement learning of mapping from symbolic representation to action with maximum expected reward over time. \textit  {Adapted from: Garnelo et al.} \cite  {Garnelo2016}.\relax }}{4}}
\newlabel{fig:dsrl_archiecture}{{1.3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions}{4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Loss functions}{7}}
\citation{Doersch2016}
\citation{Doersch2016}
\citation{Doersch2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Euclidean Distance}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax }}{8}}
\newlabel{fig:eucledian_distance_a}{{2.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \relax }}{8}}
\newlabel{fig:eucledian_distance_c}{{2.2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax }}{8}}
\newlabel{fig:eucledian_distance_b}{{2.3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textit  {Reprinted from:} \cite  {Doersch2016}\relax }}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Binary Cross-Entropy}{8}}
\citation{Zsolnai-Feher2016}
\citation{Richter2016}
\citation{Bellemare2015}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Stella}{9}}
\citation{Chollet2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Arcade Learning Environment}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Keras}{10}}
\citation{Liou2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:related_work}{{3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Autoencoders}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A black-box description of an autoencoder. The autoencoder learns the identity function, and in turn, the encoder and decoder learn suitable encoding and decoding algorithms respectively.\relax }}{11}}
\newlabel{fig:autoencoder_black_box_architecture}{{3.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Fully-Connected Autoencoders}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example architecture of a fully-connected autoencoder. The latent space is constrained by having fewer neurons than the input and output layers.\relax }}{12}}
\newlabel{fig:autoencoder_architecture}{{3.2}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A simple fully-connected autoencoder with one hidden layer. After 15 epochs, the validation score was recorded to be $71.94$.\relax }}{12}}
\newlabel{tab:fully_connected_autoencoder_architecture}{{3.1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A collection of images from the MNIST data set and their respective reconstructions using the fully-connected autoencoder specified in Table \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `tab:fully_connected_autoencoder_architecture' on page 13 undefined}. The original MNIST images are in odd columns, and their reconstructions to their immediate right.\relax }}{13}}
\newlabel{fig:fully_connected_autoencoder_mnist}{{3.3}{13}}
\citation{Krizhevsky2012}
\citation{Zeiler2014}
\citation{Szegedy2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Fully-Convolutional Autoencoders}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An example architecture of a fully-convolutional autoencoder. The latent space is constrained by reducing the number and/or size of the filters.\relax }}{14}}
\newlabel{fig:convolutional_autoencoder_architecture}{{3.4}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces A simple fully-convolutional autoencoder with 2D convolutions and max pooling, plus the corresponding deconvolutional layers. After 15 epochs, the validation score was recorded to be $64.89$.\relax }}{14}}
\newlabel{tab:convolutional_autoencoder_architecture}{{3.2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A collection of images from the MNIST data set and their respective reconstructions using the fully-convolutional autoencoder specified in Table \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `tab:convolutional_autoencoder_architecture' on page 15 undefined}. The original MNIST images are in odd columns, and their reconstructions to their immediate right.\relax }}{15}}
\newlabel{fig:convolutional_autoencoder_mnist}{{3.5}{15}}
\citation{Kingma2014}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Variational Autoencoders}{16}}
\newlabel{sec:variational_autoencoders}{{3.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Probabilistic Perspective}{16}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Generate data set $X$\relax }}{16}}
\newlabel{alg:generate_data_set_x}{{1}{16}}
\citation{Kingma2014}
\citation{Burnham2002}
\citation{Li2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Overcoming the Intractable Posterior}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Finding a Suitable Loss Function: the ELBO}{17}}
\newlabel{eq:kl_divergence_optimisation_problem}{{3.4}{18}}
\newlabel{eq:kl_divergence}{{3.9}{18}}
\citation{Blei2011}
\citation{Kingma2014}
\citation{Blei2016}
\citation{Li2016}
\newlabel{eq:elbo}{{3.14}{19}}
\newlabel{eq:elbo_optimisation_problem}{{3.17}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Writing the ELBO in Closed-Form}{19}}
\newlabel{eq:elbo_reconstruction_kl_divergence}{{3.22}{20}}
\newlabel{eq:elbo_kl_loss_plus_expectation}{{3.23}{20}}
\newlabel{eq:variational_autoencoder_prior_standard_gaussian}{{3.24}{20}}
\newlabel{eq:approximate_posterior_gaussian}{{3.25}{20}}
\citation{Li2016}
\citation{Kingma2014}
\citation{Li2016}
\citation{Li2016}
\citation{Li2016}
\citation{Kingma2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Implementing the Variational Autoencoder}{21}}
\citation{Kingma2014}
\citation{Li2016}
\citation{Li2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A na\"{i}ve implementation of the variational autoencoder. The input $\bm  {x}$ is mapped to intermediate layers taking the values of $\bm  {\mu }$ and $\bm  {\sigma }^2$. The latent variable $\bm  {z}$ is then sampled from the probabilistic encoder $\bm  {z} \sim q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$. Finally $\bm  {z}$ is mapped back to the input dimension to give reconstruction $\bm  {\mathaccentV {tilde}07E{x}}$. \textit  {Adapted from} \cite  {Li2016}.\relax }}{22}}
\newlabel{fig:variational_autoencoder_naive}{{3.6}{22}}
\newlabel{eq:reparameterisation_trick}{{3.30}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A viable implementation of the variational autoencoder. Sampling from the probabilistic encoder $q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$ is simulated by evaluating $\bm  {z} = g_{\bm  {\phi }}(\bm  {x}, \bm  {\epsilon }) = \bm  {\mu } + \bm  {\sigma } \odot \bm  {\epsilon }$.  \textit  {Adapted from} \cite  {Li2016}.\relax }}{23}}
\newlabel{fig:variational_autoencoder_reparameterisation_trick}{{3.7}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Intuition Behind the Variational Autoencoder}{23}}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The encoder takes a data point and returns a normal distribution (orange); some samples of which are shown (blue). A sample is drawn from the normal distribution (red) and decoded. \textit  {Adapted from:} \cite  {Dykeman2016}.\relax }}{25}}
\newlabel{fig:variational_autoencoder_latent_space_colour_0_with_mnist_updated}{{3.8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The prior distribution should approximate the standard multivariate Gaussian $\mathcal  {N}(\bm  {0}, \bm  {I})$. Samples of the prior are shown (yellow); two of which are decoded (red and blue).  \textit  {Adapted from:} \cite  {Dykeman2016}.\relax }}{26}}
\newlabel{fig:variational_autoencoder_latent_space_unit_gaussian_with_mnist}{{3.9}{26}}
\citation{Dykeman2016}
\citation{Dykeman2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The sum over the latent space distributions of all data points $\bm  {x}^{(i)}$ approximates the multivariate isotropic Gaussian.   \textit  {Adapted from:} \cite  {Dykeman2016}.\relax }}{27}}
\newlabel{fig:variational_autoencoder_latent_space_adding_latent_spaces_updated}{{3.10}{27}}
\citation{Thiagarajan2016}
\citation{Schmidhuber1992}
\citation{Desjardins2012}
\citation{Tang2013}
\citation{Cohen2014}
\citation{Chen2016}
\citation{Thiagarajan2016}
\citation{Chen2016}
\citation{Chen2016}
\citation{Chen2016}
\citation{Thiagarajan2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Unsupervised Learning of Generative Factors}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}InfoGAN}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces InfoGAN convincingly learns the underlying generative factors in the 3D face data set. Rows correspond to a data point and columns the value of the latent variable (varied from $-1$ to $1$). Each section (a), (b), (c) and (d) consider a different latent variable. \textit  {Source:} \cite  {Chen2016}.\relax }}{28}}
\newlabel{fig:info_gan_3d_face_dataset}{{3.11}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}$\beta $-VAE}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Derivation}{29}}
\citation{Thiagarajan2016}
\citation{Thiagarajan2016}
\citation{Creswell2016}
\newlabel{eq:beta_vae_optimisation_problem}{{3.38}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A comparison of InfoGAN, $\beta $-VAE ($\beta = 20$) and VAE on the 3D face data set. Different latent variables are varied for sections (a), (b) and (c). All models learnt lighting and elevation, but only InfoGAN and $\beta $-VAE managed to continuously vary the azimuth.  \textit  {Source:} \cite  {Thiagarajan2016}.\relax }}{31}}
\newlabel{fig:beta_vae_3d_face_dataset_comparison}{{3.12}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Improving Sampling from Generative Autoencoders with Markov Chains}{31}}
\citation{Creswell2016}
\citation{Creswell2016}
\citation{Creswell2016}
\citation{Creswell2016}
\citation{Creswell2016}
\newlabel{eq:sampling_procedure_naive}{{3.43}{32}}
\newlabel{eq:updated_sampling_mcmc}{{3.47}{32}}
\citation{Creswell2016}
\citation{Creswell2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces The probabilistic encoder $q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$ maps a given data point $\bm  {x}$ to the unknown distribution $\mathaccentV {hat}05E{p}(\bm  {z})$. The probabilistic decoder $p_{\bm  {\theta }}(\bm  {x} | \bm  {z})$ is trained to map samples from $\mathaccentV {hat}05E{p}(\bm  {z})$ back to $p(\bm  {x})$, since its inputs are drawn from the probabilistic encoder $q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$. A sample from the prior $p(\bm  {z})$ will not be mapped back by $p_{\bm  {\theta }}(\bm  {x} | \bm  {z})$ to $p(\bm  {x})$ exactly if $p(\bm  {z}) \not =\mathaccentV {hat}05E{p}(\bm  {z})$.  \textit  {Adapted from:} \cite  {Creswell2016}.\relax }}{33}}
\newlabel{fig:mcmc_sampling}{{3.13}{33}}
\newlabel{eq:mcmc_sampling}{{3.48}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Samples from a variational autoencoder trained on the CelebA data set after $t = 0, 1, 5$ and $10$ steps of the generative procedure (\G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `eq:mcmc_sampling' on page 34 undefined}). The MCMC chain was initialised with a sample from the prior $\bm  {z}_{t=0} \sim p(\bm  {z})$, which often improves the quality of the samples.  \textit  {Source:} \cite  {Creswell2016}.\relax }}{34}}
\newlabel{fig:mcmc_sampling}{{3.14}{34}}
\citation{Mnih2015}
\citation{Mnih2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:implementation}{{4}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Dimensionality Reduction}{35}}
\citation{Stokes1996}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Pre-processing Pipeline}{36}}
\@writefile{toc}{\contentsline {subsubsection}{Ensuring Object Persistence}{36}}
\@writefile{toc}{\contentsline {subsubsection}{Extracting Luminance and Cropping}{36}}
\@writefile{toc}{\contentsline {subsubsection}{File Formats}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Qualitative Assessment Using GUIs}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A collection of frames captured from Space Invaders emulated on Stella. \textbf  {Left column:} an even frame. \textbf  {Middle column:} the (odd) frame following. \textbf  {Right column:} Combining the even and odd frames by taking the maximal value over each channel (RGB). Clearly the bullets visible in one frame fail to persist in the next. As mentioned, this is due to the limited number of sprites Atari 2600 can load in a single frame.\relax }}{37}}
\newlabel{fig:even_and_odd_frames_space_invaders}{{4.1}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Pre-processed frames captured from Pong emulated on Stella. These frames were originally $84\times 84$, but are printed here as $168\times 168$ to emphasise distortions. \textbf  {Left:} The JPEG format distorts the ball, paddle and score sprites. \textbf  {Right:} The PNG format displays the frame without such distortions.\relax }}{38}}
\newlabel{fig:pong_729_pre_processed}{{4.2}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Training and Validation Data Generators}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Ensuring Numerical Stability in the Latent Space}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Activation Functions in the Latent Space}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Keras Callbacks}{39}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Methods}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:methods}{{5}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Single Latent Filter}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Architecture}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The fully-convolutional single latent filter architecture. \textbf  {Blue:} An arbitrary amount of convolutional layers. \textbf  {Green:} The latent mean $\bm  {\mu }$ and variance $\bm  {\sigma }^2$, which are both single filters of shape $(1, m, n)$. \textbf  {Orange:} A single latent filter of shape $(1, m, n)$ sampled component-wise from $\bm  {\mu }$ and $\bm  {\sigma }^2$. \textbf  {Red:} The corresponding deconvolutional layers.\relax }}{41}}
\newlabel{fig:latent_image_architecture}{{5.1}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Neuron-Level Redundancy Reduction}{41}}
\newlabel{fig:latent_image_flattening_latent_space}{{\caption@xref {fig:latent_image_flattening_latent_space}{ on input line 60}}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Multiple Latent Filters}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Architecture}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The fully-convolutional multiple latent filter architecture. \textbf  {Blue:} Unchanged. \textbf  {Green:} The latent mean $\bm  {\mu }$ and variance $\bm  {\sigma }^2$, which are both single filters of shape $(k, m, n)$. \textbf  {Orange:} A single latent filter of shape $(k, m, n)$ sampled component-wise from $\bm  {\mu }$ and $\bm  {\sigma }^2$. \textbf  {Red:} Unchanged.\relax }}{44}}
\newlabel{fig:decoupling_indiscriminately_horizontal}{{5.2}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Neuron-Level Redundancy Reduction}{44}}
\newlabel{fig:decoupling_indiscriminately_flattening_latent_space}{{\caption@xref {fig:decoupling_indiscriminately_flattening_latent_space}{ on input line 116}}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Na{\"i}ve Filter-Level Redundancy Reduction}{45}}
\newlabel{fig:decoupling_averages_latent_space}{{\caption@xref {fig:decoupling_averages_latent_space}{ on input line 138}}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Weighted Filter-Level Redundancy Reduction}{45}}
\newlabel{fig:decoupling_weighted_averages_latent_space}{{\caption@xref {fig:decoupling_weighted_averages_latent_space}{ on input line 159}}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Winner Takes All}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Multiple types of objects recognised in same position.\relax }}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces At most one object recognised in a given position.\relax }}{47}}
\newlabel{fig:winner_takes_all_activations_on_layers}{{\caption@xref {fig:winner_takes_all_activations_on_layers}{ on input line 187}}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Position-Wise Redundancy Reduction}{47}}
\newlabel{fig:winner_takes_all_line_through_object_on_every_filter}{{\caption@xref {fig:winner_takes_all_line_through_object_on_every_filter}{ on input line 205}}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Separating Colour Spaces}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Original\relax }}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Red\relax }}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Green\relax }}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Blue\relax }}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces A comparison of a $210 \times 160$ RGB frame from Space Invaders and its red, green and blue channels. The bullet is clearly separated from other sprites in the blue channel. The red and green channels separate collections of sprites. The red channel excludes the gunship and players score, while the green partially excludes the barriers.\relax }}{48}}
\newlabel{fig:separating_colour_spaces}{{5.9}{48}}
\citation{Higgins2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{49}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Results}{{6}{49}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Architectures}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Fully-convolutional single-filter variational autoencoder.\relax }}{50}}
\newlabel{tab:fully_convolutional_single_filter}{{6.1}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Fully-convolutional multiple latent filter variational autoencoder.\relax }}{50}}
\newlabel{tab:fully_convolutional_multiple_filter}{{6.2}{50}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Fully-convolutional multiple latent filter variational autoencoder for RGB images.\relax }}{50}}
\newlabel{tab:fully_convolutional_multiple_filter_rgb}{{6.3}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Single Latent Filter}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Results}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Summary}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \textbf  {Single latent filter architecture}. The validation, validation KL and validation reconstruction loss for the latent image architecture and different values of $\beta $.\relax }}{52}}
\newlabel{fig:latent_image_graphs}{{6.1}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Original\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces $\beta = 1$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces $\beta = 4$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces $\beta = 16$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Original\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces $\beta = 1$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces $\beta = 4$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces $\beta = 16$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Original\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces $\beta = 1$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces $\beta = 4$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces $\beta = 16$\relax }}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces \textbf  {Single latent filter architecture}. A selection of Space Invader frames and their corresponding reconstructions for different values of $\beta $.\relax }}{53}}
\newlabel{fig:latent_image_originals_and_reconstructions}{{6.14}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Original\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces $\beta =1$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces $\beta =4$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces $\beta =16$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Original\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces $\beta =1$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces $\beta =4$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces $\beta =16$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces Original\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces $\beta =1$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces $\beta =4$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces $\beta =16$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces \textbf  {Single latent filter architecture}. Frames from Space Invaders and their corresponding latent images for different values of $\beta $.\relax }}{54}}
\newlabel{fig:latent_image_originals_and_latent_spaces}{{6.27}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.28}{\ignorespaces $\beta =1$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.29}{\ignorespaces $\beta =4$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.30}{\ignorespaces $\beta =16$\relax }}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.31}{\ignorespaces \textbf  {Single latent filter architecture}. The best of $10$ samples from the prior $p(\bm  {z}) = \mathcal  {N}(\bm  {0}, \bm  {I})$ for different values of $\beta $.\relax }}{54}}
\newlabel{fig:latent_image_originals_prior_samples}{{6.31}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.32}{\ignorespaces Original\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.33}{\ignorespaces $\beta =1$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.34}{\ignorespaces $\beta =4$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.35}{\ignorespaces $\beta =16$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.36}{\ignorespaces \textbf  {Single latent filter architecture}. Samples from the unknown distribution $\mathaccentV {hat}05E{p}(\bm  {z})$ after one step of MCMC for different values of $\beta $. Samples for subsequent steps did not improve, hence we only include the first for each value of $\beta $.\relax }}{55}}
\newlabel{fig:latent_image_originals_posterior_samples}{{6.36}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.37}{\ignorespaces $\beta =1$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.38}{\ignorespaces $\beta =4$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.39}{\ignorespaces $\beta =16$\relax }}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.40}{\ignorespaces \textbf  {Single latent filter architecture}. An average of the activations in the latent image over $10,000$ images from the test set for different values of $\beta $.\relax }}{55}}
\newlabel{fig:latent_image_average_filters}{{6.40}{55}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Neuron-Level Redundancy Reduction}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Results}{56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Summary}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.41}{\ignorespaces \textbf  {Multiple latent filter archiecture with neuron-level redundancy reduction}. The validation, validation KL and validation reconstruction loss for the latent image architecture and different values of $\beta $.\relax }}{57}}
\newlabel{fig:indiscriminate_decoupling_graphs}{{6.41}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.42}{\ignorespaces Original\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.43}{\ignorespaces $\beta = 1$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.44}{\ignorespaces $\beta = 4$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.45}{\ignorespaces $\beta = 32$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.46}{\ignorespaces Original\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.47}{\ignorespaces $\beta = 1$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.48}{\ignorespaces $\beta = 4$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.49}{\ignorespaces $\beta = 32$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.50}{\ignorespaces Original\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.51}{\ignorespaces $\beta = 1$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.52}{\ignorespaces $\beta = 4$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.53}{\ignorespaces $\beta = 32$\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.54}{\ignorespaces \textbf  {Multiple latent filter architecture with neuron-level redundancy reduction}. A selection of Space Invader frames and their corresponding reconstructions for different values of $\beta $.\relax }}{58}}
\newlabel{fig:indiscriminate_decoupling_originals_and_reconstructions}{{6.54}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.55}{\ignorespaces Original\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.56}{\ignorespaces $\beta = 1$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.57}{\ignorespaces $\beta = 4$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.58}{\ignorespaces $\beta = 32$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.59}{\ignorespaces \textbf  {Multiple latent filter architecture with neuron-level redundancy reduction}. A Space Invaders frame and the activations over its corresponding latent filters for different values of $\beta $.\relax }}{59}}
\newlabel{fig:indiscriminate_decoupling_originals_and_latent_filters}{{6.59}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.60}{\ignorespaces $\beta =1$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.61}{\ignorespaces $\beta =4$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.62}{\ignorespaces $\beta =32$\relax }}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.63}{\ignorespaces \textbf  {Multiple latent filter architecture with neuron-level redundancy reduction}. The best of $10$ samples from the prior $p(\bm  {z}) = \mathcal  {N}(\bm  {0}, \bm  {I})$ for different values of $\beta $.\relax }}{59}}
\newlabel{fig:indiscriminate_decoupling_prior_samples}{{6.63}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.64}{\ignorespaces $\beta =1\hskip 1em\relax $ (original)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.65}{\ignorespaces $\beta =1\hskip 1em\relax $ (19 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.66}{\ignorespaces $\beta =1\hskip 1em\relax $ (26 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.67}{\ignorespaces $\beta =1\hskip 1em\relax $ (60 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.68}{\ignorespaces $\beta =4\hskip 1em\relax $ (original)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.69}{\ignorespaces $\beta =4\hskip 1em\relax $ (1 step)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.70}{\ignorespaces $\beta =4\hskip 1em\relax $ (7 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.71}{\ignorespaces $\beta =4\hskip 1em\relax $ (98 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.72}{\ignorespaces $\beta =32\hskip 1em\relax $ (original)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.73}{\ignorespaces $\beta =32\hskip 1em\relax $ (1 step)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.74}{\ignorespaces $\beta =32\hskip 1em\relax $ (15 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.75}{\ignorespaces $\beta =32\hskip 1em\relax $ (48 steps)\relax }}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.76}{\ignorespaces \textbf  {Multiple latent filter architecture with neuron-level redundancy reduction}. A selection of Space Invader frames and the following samples from the unknown prior $\mathaccentV {hat}05E{p}(\bm  {z})$ using MCMC for different values of $\beta $.\relax }}{60}}
\newlabel{fig:indiscriminate_decoupling_posterior_samples}{{6.76}{60}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.77}{\ignorespaces $\beta =1$\relax }}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.78}{\ignorespaces $\beta =4$\relax }}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.79}{\ignorespaces $\beta =32$\relax }}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.80}{\ignorespaces \textbf  {Multiple latent filter architecture with neuron-level redundancy reduction}. An average of the activations in the latent image over $10,000$ images from the test set for different values of $\beta $.\relax }}{61}}
\newlabel{fig:indiscriminate_decoupling_average_filters}{{6.80}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Na{\"i}ve Filter-Level Redundancy Reduction}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Results}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Summary}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.81}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. The validation, validation KL and validation reconstruction loss for the latent image architecture and different values of $\beta $. The validation reconstruction loss for $\beta = 16, 32$ were $\sim 950$, and are therefore excluded in two plots for readability.\relax }}{63}}
\newlabel{fig:naive_average_graphs}{{6.81}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.82}{\ignorespaces Original\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.83}{\ignorespaces $\beta = 1$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.84}{\ignorespaces $\beta = 2$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.85}{\ignorespaces $\beta = 32$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.86}{\ignorespaces Original\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.87}{\ignorespaces $\beta = 1$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.88}{\ignorespaces $\beta = 2$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.89}{\ignorespaces $\beta = 4$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.90}{\ignorespaces Original\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.91}{\ignorespaces $\beta = 1$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.92}{\ignorespaces $\beta = 2$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.93}{\ignorespaces $\beta = 4$\relax }}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.94}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. A selection of Space Invader frames and their corresponding reconstructions for different values of $\beta $.\relax }}{64}}
\newlabel{fig:naive_average_originals_and_reconstructions}{{6.94}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.95}{\ignorespaces Original\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.96}{\ignorespaces $\beta = 1$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.97}{\ignorespaces $\beta = 4$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.98}{\ignorespaces $\beta = 32$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.99}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. A Space Invaders frame and the activations over its corresponding latent filters for different values of $\beta $.\relax }}{65}}
\newlabel{fig:naive_average_originals_and_latent_filters}{{6.99}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.100}{\ignorespaces $\beta =1$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.101}{\ignorespaces $\beta =4$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.102}{\ignorespaces $\beta =32$\relax }}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.103}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. The best of $10$ samples from the prior $p(\bm  {z}) = \mathcal  {N}(\bm  {0}, \bm  {I})$ for different values of $\beta $.\relax }}{65}}
\newlabel{fig:naive_average_decoupling_prior_samples}{{6.103}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.104}{\ignorespaces $\beta =1\hskip 1em\relax $ (original)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.105}{\ignorespaces $\beta =1\hskip 1em\relax $ (1 step)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.106}{\ignorespaces $\beta =1\hskip 1em\relax $ (5 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.107}{\ignorespaces $\beta =1\hskip 1em\relax $ (10 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.108}{\ignorespaces $\beta =4\hskip 1em\relax $ (original)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.109}{\ignorespaces $\beta =4\hskip 1em\relax $ (1 step)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.110}{\ignorespaces $\beta =4\hskip 1em\relax $ (5 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.111}{\ignorespaces $\beta =4\hskip 1em\relax $ (10 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.112}{\ignorespaces $\beta =32\hskip 1em\relax $ (original)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.113}{\ignorespaces $\beta =32\hskip 1em\relax $ (1 step)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.114}{\ignorespaces $\beta =32\hskip 1em\relax $ (5 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.115}{\ignorespaces $\beta =32\hskip 1em\relax $ (10 steps)\relax }}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.116}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. A selection of Space Invader frames and the following samples from the unknown prior $\mathaccentV {hat}05E{p}(\bm  {z})$ using MCMC for different values of $\beta $.\relax }}{66}}
\newlabel{fig:naive_average_originals_posterior_samples}{{6.116}{66}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.117}{\ignorespaces $\beta =1$\relax }}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.118}{\ignorespaces $\beta =4$\relax }}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.119}{\ignorespaces $\beta =32$\relax }}{67}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.120}{\ignorespaces \textbf  {Multiple latent filter architecture with na{\"i}ve filter-level redundancy reduction}. An average of the activations in the latent image over $10,000$ images from the test set for different values of $\beta $.\relax }}{67}}
\newlabel{fig:naive_average_average_filters}{{6.120}{67}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Weighted Filter-Level Redundancy Reduction}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Results}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Summary}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.121}{\ignorespaces \textbf  {Multiple latent filter architecture with weighted filter-level redundancy reduction}. The validation, validation KL and validation reconstruction loss for the latent image architecture and different values of $\beta $.\relax }}{69}}
\newlabel{fig:weighted_average_graphs}{{6.121}{69}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.122}{\ignorespaces Original\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.123}{\ignorespaces $\beta = 1$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.124}{\ignorespaces $\beta = 4$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.125}{\ignorespaces $\beta = 32$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.126}{\ignorespaces Original\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.127}{\ignorespaces $\beta = 1$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.128}{\ignorespaces $\beta = 4$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.129}{\ignorespaces $\beta = 32$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.130}{\ignorespaces Original\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.131}{\ignorespaces $\beta = 1$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.132}{\ignorespaces $\beta = 4$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.133}{\ignorespaces $\beta = 32$\relax }}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.134}{\ignorespaces \textbf  {Multiple latent filter architecture with weighted filter-level redundancy reduction}. A selection of Space Invader frames and their corresponding reconstructions for different values of $\beta $.\relax }}{70}}
\newlabel{fig:weighted_average_originals_and_reconstructions}{{6.134}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.135}{\ignorespaces Original\relax }}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.136}{\ignorespaces $\beta = 1$\relax }}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.137}{\ignorespaces $\beta = 4$\relax }}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.138}{\ignorespaces $\beta = 32$\relax }}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.139}{\ignorespaces \textbf  {Multiple latent filter architecture with weighted filter-level redundancy reduction}. A Space Invaders frame and the activations over its corresponding latent filters for different values of $\beta $.\relax }}{71}}
\newlabel{fig:weighted_average_originals_and_latent_filters}{{6.139}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.140}{\ignorespaces $\beta =1\hskip 1em\relax $ (original)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.141}{\ignorespaces $\beta =1\hskip 1em\relax $ (18 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.142}{\ignorespaces $\beta =1\hskip 1em\relax $ (26 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.143}{\ignorespaces $\beta =1\hskip 1em\relax $ (39 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.144}{\ignorespaces $\beta =4\hskip 1em\relax $ (original)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.145}{\ignorespaces $\beta =4\hskip 1em\relax $ (1 step)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.146}{\ignorespaces $\beta =4\hskip 1em\relax $ (5 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.147}{\ignorespaces $\beta =4\hskip 1em\relax $ (40 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.148}{\ignorespaces $\beta =32\hskip 1em\relax $ (original)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.149}{\ignorespaces $\beta =32\hskip 1em\relax $ (1 step)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.150}{\ignorespaces $\beta =32\hskip 1em\relax $ (47 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.151}{\ignorespaces $\beta =32\hskip 1em\relax $ (62 steps)\relax }}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.152}{\ignorespaces \textbf  {Multiple latent filter architecture with weighted filter-level redundancy reduction}. A selection of Space Invader frames and the following samples from the unknown prior $\mathaccentV {hat}05E{p}(\bm  {z})$ using MCMC for different values of $\beta $.\relax }}{72}}
\newlabel{fig:weighted_average_posterior_samples}{{6.152}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.153}{\ignorespaces $\beta =1$\relax }}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.154}{\ignorespaces $\beta =2$\relax }}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.155}{\ignorespaces $\beta =4$\relax }}{73}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.156}{\ignorespaces \textbf  {Multiple latent filter architecture with weighted filter-level redundancy reduction}. An average of the activations in the latent image over $10,000$ images from the test set for different values of $\beta $.\relax }}{73}}
\newlabel{fig:weighted_average_average_filters}{{6.156}{73}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Separating Colour Spaces}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Results}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Summary}{74}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.157}{\ignorespaces Original\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.158}{\ignorespaces $\beta = 1$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.159}{\ignorespaces $\beta = 2$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.160}{\ignorespaces $\beta = 4$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.161}{\ignorespaces Original\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.162}{\ignorespaces $\beta = 1$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.163}{\ignorespaces $\beta = 2$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.164}{\ignorespaces $\beta = 4$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.165}{\ignorespaces Original\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.166}{\ignorespaces $\beta = 1$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.167}{\ignorespaces $\beta = 2$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.168}{\ignorespaces $\beta = 4$\relax }}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.169}{\ignorespaces \textbf  {Multiple latent filter architecture for RGB with weighted filter-level redundancy reduction}. A selection of Space Invader frames and their corresponding reconstructions for different values of $\beta $.\relax }}{75}}
\newlabel{fig:colour_separated_originals_and_reconstructions}{{6.169}{75}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.170}{\ignorespaces Original\relax }}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.171}{\ignorespaces $\beta = 1$\relax }}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.172}{\ignorespaces $\beta = 2$\relax }}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.173}{\ignorespaces $\beta = 4$\relax }}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.174}{\ignorespaces \textbf  {Multiple latent filter architecture for RGB with weighted filter-level redundancy reduction}. A Space Invaders frame and the activations over its corresponding latent filters for different values of $\beta $.\relax }}{76}}
\newlabel{fig:colour_separated_originals_and_latent_filters}{{6.174}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.175}{\ignorespaces $\beta =1\hskip 1em\relax $ (original)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.176}{\ignorespaces $\beta =1\hskip 1em\relax $ (7 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.177}{\ignorespaces $\beta =1\hskip 1em\relax $ (17 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.178}{\ignorespaces $\beta =1\hskip 1em\relax $ (18 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.179}{\ignorespaces $\beta =2\hskip 1em\relax $ (original)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.180}{\ignorespaces $\beta =2\hskip 1em\relax $ (10 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.181}{\ignorespaces $\beta =2\hskip 1em\relax $ (17 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.182}{\ignorespaces $\beta =2\hskip 1em\relax $ (31 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.183}{\ignorespaces $\beta =4\hskip 1em\relax $ (original)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.184}{\ignorespaces $\beta =4\hskip 1em\relax $ (1 step)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.185}{\ignorespaces $\beta =4\hskip 1em\relax $ (3 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.186}{\ignorespaces $\beta =4\hskip 1em\relax $ (20 steps)\relax }}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.187}{\ignorespaces \textbf  {Multiple latent filter architecture for RGB with weighted filter-level redundancy reduction}. A selection of Space Invader frames and the following samples from the unknown prior $\mathaccentV {hat}05E{p}(\bm  {z})$ using MCMC for different values of $\beta $.\relax }}{77}}
\newlabel{fig:colour_separated_posterior_samples}{{6.187}{77}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.188}{\ignorespaces $\beta =1$\relax }}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.189}{\ignorespaces $\beta =2$\relax }}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.190}{\ignorespaces $\beta =4$\relax }}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.191}{\ignorespaces \textbf  {Multiple latent filter architecture for RGB with weighted filter-level redundancy reduction}. An average of the activations in the latent image over $10,000$ images from the test set for different values of $\beta $.\relax }}{78}}
\newlabel{fig:colour_separated_average_filters}{{6.191}{78}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{79}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{7}{79}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Thesis Achievements}{79}}
\citation{Rodriguez2016}
\bibstyle{abbrv}
\bibdata{library}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future Work}{80}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{80}}
\bibcite{Bellemare2015}{1}
\bibcite{Blei2011}{2}
\bibcite{Blei2016}{3}
\bibcite{Burnham2002}{4}
\bibcite{Chen2016}{5}
\bibcite{Chollet2016}{6}
\bibcite{Cohen2014}{7}
\bibcite{Creswell2016}{8}
\bibcite{Darrach}{9}
\bibcite{Desjardins2012}{10}
\bibcite{Doersch2016}{11}
\bibcite{Dykeman2016}{12}
\bibcite{Garnelo2016}{13}
\bibcite{Higgins2016}{14}
\bibcite{Hutter2005}{15}
\bibcite{Kingma2014}{16}
\bibcite{Knight2016}{17}
\bibcite{Krizhevsky2012}{18}
\bibcite{Li2016}{19}
\bibcite{Liou2008}{20}
\bibcite{Mnih2015}{21}
\bibcite{Ormerod2016}{22}
\bibcite{Reingold2001}{23}
\bibcite{Richter2016}{24}
\bibcite{Rodriguez2016}{25}
\bibcite{Rosen2012}{26}
\bibcite{Schmidhuber1992}{27}
\bibcite{Silver2016}{28}
\bibcite{Stokes1996}{29}
\bibcite{Szegedy2015}{30}
\bibcite{Tang2013}{31}
\bibcite{Thiagarajan2016}{32}
\bibcite{Zeiler2014}{33}
\bibcite{Zsolnai-Feher2016}{34}
