\relax 
\@writefile{toc}{\contentsline {chapter}{Acknowledgements}{i}}
\citation{Rosen2012}
\citation{Ormerod2016}
\citation{Garnelo2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Chen2016}
\citation{Thiagarajan2016}
\citation{Knight2016}
\citation{Darrach}
\citation{Reingold2001}
\citation{Garnelo2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}}
\citation{Hutter2005}
\citation{Mnih2015}
\citation{Silver2016}
\citation{Rosen2012}
\citation{Rosen2012}
\citation{Ormerod2016}
\citation{Ormerod2016}
\citation{Garnelo2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces May 1997: Gary Kasparov makes his first move against IBM's Deep Blue. Deep Blue would later emerge the victor in the best of six games; the first time a reigning world chess champion is defeated by a computer. \cite  {Rosen2012}\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:deep_blue_kasparov}{{1.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces March 2016: Lee Sedol, one of the greatest modern Go players, plays his first move of game three against AlphaGo. AlphaGo won four of five games. This feat was considered by many to be a decade away. \cite  {Ormerod2016}\relax }}{2}}
\newlabel{fig:alpha_go_first_move_game_three}{{1.2}{2}}
\citation{Garnelo2016}
\citation{Garnelo2016}
\citation{Garnelo2016}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Overview of deep symbolic reinforcement learning system architecture. \textbf  {A}: The neural back end maps high-dimensional raw input data to a compositionally structured symbolic representation. \textbf  {B}: The compositionally structured symbolic representation. \textbf  {C}: Reinforcement learning of mapping from symbolic representation to action with maximum expected reward over time. \textit  {Source: Garnelo et al.} \cite  {Garnelo2016}.\relax }}{4}}
\newlabel{fig:dsrl_archiecture}{{1.3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Objectives}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces A toy example of a raw high-dimensional input.\relax }}{4}}
\newlabel{fig:dsrl_raw_input}{{1.4}{4}}
\newlabel{tab:dsrl_symbolic_representation}{{\caption@xref {tab:dsrl_symbolic_representation}{ on input line 84}}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Low-dimensional symbolic representation\relax }}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Loss functions}{6}}
\citation{Doersch2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Euclidean Distance}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax }}{7}}
\newlabel{fig:eucledian_distance_a}{{2.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \relax }}{7}}
\newlabel{fig:eucledian_distance_c}{{2.2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax }}{7}}
\newlabel{fig:eucledian_distance_b}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Binary Cross-Entropy}{7}}
\citation{Zsolnai-Feher2016}
\citation{Richter2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Stella}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Arcade Learning Environment}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Keras}{8}}
\citation{Liou2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:related_work}{{3}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Autoencoders}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A black-box description of an autoencoder. The autoencoder learns the identity function, and in turn, the encoder and decoder learn suitable encoding and decoding algorithms respectively.\relax }}{9}}
\newlabel{fig:autoencoder_black_box_architecture}{{3.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Fully-Connected Autoencoders}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces An example architecture of a fully-connected autoencoder. The latent space is constrained by having fewer neurons than the input and output layers.\relax }}{10}}
\newlabel{fig:autoencoder_architecture}{{3.2}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A simple fully-connected autoencoder with one hidden layer. After 15 epochs, the validation score was recorded to be $71.94$.\relax }}{10}}
\newlabel{tab:fully_connected_autoencoder_architecture}{{3.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A collection of images from the MNIST data set and their respective reconstructions using the fully-connected autoencoder specified in Table 3.1\hbox {}. The original MNIST images are in odd columns, and their reconstructions to their immediate right.\relax }}{11}}
\newlabel{fig:fully_connected_autoencoder_mnist}{{3.3}{11}}
\citation{Krizhevsky2012}
\citation{Zeiler2014}
\citation{Szegedy2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Fully-Convolutional Autoencoders}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces An example architecture of a fully-convolutional autoencoder. The latent space is constrained by reducing the number and/or size of the filters.\relax }}{12}}
\newlabel{fig:convolutional_autoencoder_architecture}{{3.4}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces A simple fully-convolutional autoencoder with 2D convolutions and max pooling, plus the corresponding deconvolutional layers. After 15 epochs, the validation score was recorded to be $64.89$.\relax }}{12}}
\newlabel{tab:convolutional_autoencoder_architecture}{{3.2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A collection of images from the MNIST data set and their respective reconstructions using the fully-convolutional autoencoder specified in Table 3.2\hbox {}. The original MNIST images are in odd columns, and their reconstructions to their immediate right.\relax }}{13}}
\newlabel{fig:convolutional_autoencoder_mnist}{{3.5}{13}}
\citation{Kingma2014}
\citation{Kingma2014}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Variational Autoencoders}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}A Probabilistic Perspective}{14}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Generate data set $X$\relax }}{14}}
\newlabel{alg:generate_data_set_x}{{1}{14}}
\citation{Kingma2014}
\citation{Burnham2002}
\citation{Li2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Overcoming the Intractable Posterior}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Finding a Suitable Loss Function: the ELBO}{15}}
\newlabel{eq:kl_divergence_optimisation_problem}{{3.4}{16}}
\newlabel{eq:kl_divergence}{{3.9}{16}}
\citation{Blei2011}
\citation{Kingma2014}
\citation{Blei2016}
\citation{Li2016}
\newlabel{eq:elbo}{{3.14}{17}}
\newlabel{eq:elbo_optimisation_problem}{{3.17}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Writing the ELBO in Closed-Form}{17}}
\newlabel{eq:elbo_reconstruction_kl_divergence}{{3.22}{18}}
\newlabel{eq:elbo_kl_loss_plus_expectation}{{3.23}{18}}
\newlabel{eq:variational_autoencoder_prior_standard_gaussian}{{3.24}{18}}
\newlabel{eq:approximate_posterior_gaussian}{{3.25}{18}}
\citation{Li2016}
\citation{Kingma2014}
\citation{Li2016}
\citation{Kingma2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Implementing the Variational Autoencoder}{19}}
\citation{Kingma2014}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces A na\"{i}ve implementation of the variational autoencoder. The input $\bm  {x}$ is mapped to intermediate layers taking the values of $\bm  {\mu }$ and $\bm  {\sigma }^2$. The latent variable $\bm  {z}$ is then sampled from the probabilistic encoder $\bm  {z} \sim q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$. Finally $\bm  {z}$ is mapped back to the input dimension to give reconstruction $\bm  {\mathaccentV {tilde}07E{x}}$.\relax }}{20}}
\newlabel{fig:variational_autoencoder_naive}{{3.6}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A viable implementation of the variational autoencoder. Sampling from the probabilistic encoder $q_{\bm  {\phi }}(\bm  {z} | \bm  {x})$ is simulated by evaluating $\bm  {z} = g_{\bm  {\phi }}(\bm  {x}, \bm  {\epsilon }) = \bm  {\mu } + \bm  {\sigma } \odot \bm  {\epsilon }$.\relax }}{21}}
\newlabel{fig:variational_autoencoder_reparameterisation_trick}{{3.7}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Intuition Behind the Variational Autoencoder}{21}}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\citation{Dykeman2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The encoder takes a data point and returns a normal distribution (orange); some samples of which are shown (blue). A sample is drawn from the normal distribution (red) and decoded. \cite  {Dykeman2016}\relax }}{23}}
\newlabel{fig:variational_autoencoder_latent_space_colour_0_with_mnist_updated}{{3.8}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The prior distribution should approximate the standard multivariate Gaussian $\mathcal  {N}(\bm  {0}, \bm  {I})$. Samples of the prior are shown (yellow); two of which are decoded (red and blue). \cite  {Dykeman2016}\relax }}{24}}
\newlabel{fig:variational_autoencoder_latent_space_unit_gaussian_with_mnist}{{3.9}{24}}
\citation{Dykeman2016}
\citation{Dykeman2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces \vspace  *{-20mm}The sum over the latent space distributions of all data points $\bm  {x}^{(i)}$ approximates the multivariate isotropic Gaussian. \cite  {Dykeman2016}\relax }}{25}}
\newlabel{fig:variational_autoencoder_latent_space_adding_latent_spaces_updated}{{3.10}{25}}
\citation{Higgins2016}
\citation{Thiagarajan2016}
\citation{Thiagarajan2016}
\citation{Schmidhuber1992}
\citation{Desjardins2012}
\citation{Tang2013}
\citation{Cohen2014}
\citation{Chen2016}
\citation{Thiagarajan2016}
\citation{Chen2016}
\citation{Chen2016}
\citation{Chen2016}
\citation{Thiagarajan2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Disentangled Representations}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Unsupervised Learning of Generative Factors}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}InfoGAN}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}$\beta $-VAE}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces InfoGAN convincingly learns the underlying generative factors in the 3D face data set. Rows correspond to a data point and columns the value of the latent variable (varied from $-1$ to $1$). Each section (a), (b), (c) and (d) consider a different latent variable. \cite  {Chen2016}\relax }}{27}}
\newlabel{fig:info_gan_3d_face_dataset}{{3.11}{27}}
\@writefile{toc}{\contentsline {subsubsection}{Derivation}{27}}
\newlabel{eq:beta_vae_optimisation_problem}{{3.35}{28}}
\citation{Thiagarajan2016}
\citation{Thiagarajan2016}
\citation{Mnih2015}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces A comparison between InfoGAN, $\beta $-VAE and VAE on the 3D face data set for $\beta = 20$. Different latent variables are varied for sections (a), (b) and (c). All models learnt lighting and elevation, but only InfoGAN and $\beta $-VAE managed to continuously vary the azimuth. \cite  {Thiagarajan2016}\relax }}{29}}
\newlabel{fig:beta_vae_3d_face_dataset_comparison}{{3.12}{29}}
\citation{Mnih2015}
\citation{Creswell2016}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Human-Level Control Through Deep Reinforcement Learning}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Pre-processing Stella Frames}{30}}
\@writefile{toc}{\contentsline {subsubsection}{Sprite Rendering in Stella}{30}}
\@writefile{toc}{\contentsline {subsubsection}{Extracting Luminescence}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces A collection of frames captured from Space Invaders emulated on Stella. \textbf  {Left column:} an even frame. \textbf  {Middle column:} the (odd) frame following. \textbf  {Right column:} Combining the even and odd frames by taking the maximal value over each channel (RGB). Clearly the bullets visible in one frame fail to persist in the next. As discussed, this is due to the limited number of sprites Atari 2600 can load in a single frame.\relax }}{31}}
\newlabel{fig:even_and_odd_frames_space_invaders}{{3.13}{31}}
\citation{Rodriguez2016}
\@writefile{toc}{\contentsline {subsubsection}{Cropping}{32}}
\@writefile{toc}{\contentsline {subsubsection}{File Formats}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Improving Sampling from Generative Autoencoders with Markov Chains}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Regularizing CNNs with Locally Constrained Decorrelations}{32}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Method}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:method}{{4}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Psuedo-Dense Latent Space}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Latent Image}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Winner Takes All}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Orthogonal Convolutions}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Separating Colour Spaces}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Average Filters}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Average-Weighted Filters}{33}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Results}{{5}{34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:Discussion}{{6}{35}}
\bibstyle{abbrv}
\bibdata{library}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{36}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusions}{{7}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Thesis Achievements}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Applications}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Future Work}{36}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{36}}
\bibcite{Blei2011}{1}
\bibcite{Blei2016}{2}
\bibcite{Burnham2002}{3}
\bibcite{Chen2016}{4}
\bibcite{Cohen2014}{5}
\bibcite{Creswell2016}{6}
\bibcite{Darrach}{7}
\bibcite{Desjardins2012}{8}
\bibcite{Doersch2016}{9}
\bibcite{Dykeman2016}{10}
\bibcite{Garnelo2016}{11}
\bibcite{Higgins2016}{12}
\bibcite{Hutter2005}{13}
\bibcite{Kingma2014}{14}
\bibcite{Knight2016}{15}
\bibcite{Krizhevsky2012}{16}
\bibcite{Li2016}{17}
\bibcite{Liou2008}{18}
\bibcite{Mnih2015}{19}
\bibcite{Ormerod2016}{20}
\bibcite{Reingold2001}{21}
\bibcite{Richter2016}{22}
\bibcite{Rosen2012}{23}
\bibcite{Schmidhuber1992}{24}
\bibcite{Silver2016}{25}
\bibcite{Szegedy2015}{26}
\bibcite{Tang2013}{27}
\bibcite{Thiagarajan2016}{28}
\bibcite{Zeiler2014}{29}
\bibcite{Zsolnai-Feher2016}{30}
