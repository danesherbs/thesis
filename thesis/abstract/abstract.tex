\addcontentsline{toc}{chapter}{Abstract}

\begin{abstract}

Reinforcement learning has achieved considerable success in recent years. One significant breakthrough was Google DeepMind's deep Q-network (DQN), which mastered a wide range of Atari 2600 games to a super-human level using only the pixels and score. However, reinforcement learning agents lack essential properties necessary for artificial general intelligence (AGI); namely they are slow to learn, unable to transfer knowledge between similar tasks and are unable to reason abstractly. In this thesis we seek to advance a method likely to overcome these drawbacks, known as deep symbolic reinforcement learning (DSRL). By applying recent advancements in the unsupervised learning of generative factors to fully-convolutional variational autoencoders, we develop a first iteration solution for a scalable DSRL system.

%The recent invention of deep symbolic reinforcement learning (DSRL), which is a marrying of deep reinforcement learning and symbolic logic, is a promising approach to overcome these drawbacks. However, this framework relies on the extraction of object types and their position in an unsupervised manner, which is currently an open problem in AI research. In this project, we pair a recent development of a scalable unsupervised method of learning generative factors in complex scenes, known as $\beta$-VAE, to fully-convolutional variational autoencoders in an attempt to preserve the object type and spatial information necessary for DSRL systems.


\end{abstract}