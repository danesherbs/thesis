\contentsline {chapter}{Acknowledgements}{i}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Motivation}{1}
\contentsline {section}{\numberline {1.2}Objectives}{4}
\contentsline {section}{\numberline {1.3}Contributions}{5}
\contentsline {chapter}{\numberline {2}Background}{6}
\contentsline {section}{\numberline {2.1}Loss functions}{6}
\contentsline {subsection}{\numberline {2.1.1}Euclidean Distance}{7}
\contentsline {subsection}{\numberline {2.1.2}Binary Cross-Entropy}{7}
\contentsline {section}{\numberline {2.2}Stella}{8}
\contentsline {section}{\numberline {2.3}Arcade Learning Environment}{8}
\contentsline {section}{\numberline {2.4}Keras}{9}
\contentsline {section}{\numberline {2.5}TensorBoard}{9}
\contentsline {chapter}{\numberline {3}Related Work}{10}
\contentsline {section}{\numberline {3.1}Autoencoders}{10}
\contentsline {subsection}{\numberline {3.1.1}Fully-Connected Autoencoders}{11}
\contentsline {subsection}{\numberline {3.1.2}Fully-Convolutional Autoencoders}{13}
\contentsline {section}{\numberline {3.2}Variational Autoencoders}{15}
\contentsline {subsection}{\numberline {3.2.1}A Probabilistic Perspective}{15}
\contentsline {subsection}{\numberline {3.2.2}Overcoming the Intractable Posterior}{16}
\contentsline {subsection}{\numberline {3.2.3}Finding a Suitable Loss Function: the ELBO}{16}
\contentsline {subsection}{\numberline {3.2.4}Writing the ELBO in Closed-Form}{18}
\contentsline {subsection}{\numberline {3.2.5}Implementing the Variational Autoencoder}{20}
\contentsline {subsection}{\numberline {3.2.6}Intuition Behind the Variational Autoencoder}{22}
\contentsline {section}{\numberline {3.3}Disentangled Representations}{27}
\contentsline {section}{\numberline {3.4}Unsupervised Learning of Generative Factors}{27}
\contentsline {subsection}{\numberline {3.4.1}InfoGAN}{27}
\contentsline {subsection}{\numberline {3.4.2}$\beta $-VAE}{27}
\contentsline {subsubsection}{Derivation}{28}
\contentsline {section}{\numberline {3.5}Improving Sampling from Generative Autoencoders with Markov Chains}{31}
\contentsline {section}{\numberline {3.6}Regularizing CNNs with Locally Constrained Decorrelations}{33}
\contentsline {section}{\numberline {3.7}Batch Normalisation}{33}
\contentsline {chapter}{\numberline {4}Implementation}{34}
\contentsline {section}{\numberline {4.1}Dimensionality Reduction}{34}
\contentsline {subsection}{\numberline {4.1.1}Pre-processing Pipeline}{35}
\contentsline {subsubsection}{Ensuring Object Persistence}{35}
\contentsline {subsubsection}{Extracting Luminance and Cropping}{35}
\contentsline {subsubsection}{File Formats}{35}
\contentsline {section}{\numberline {4.2}Qualitative Assessment Using GUIs}{35}
\contentsline {section}{\numberline {4.3}Ensuring Numerical Stability in the Latent Space}{37}
\contentsline {section}{\numberline {4.4}Activation Functions in the Latent Space}{37}
\contentsline {section}{\numberline {4.5}Keras Callbacks}{38}
\contentsline {chapter}{\numberline {5}Methods}{39}
\contentsline {section}{\numberline {5.1}Single Latent Filter}{40}
\contentsline {subsection}{\numberline {5.1.1}Architecture}{40}
\contentsline {subsection}{\numberline {5.1.2}Derivation}{41}
\contentsline {section}{\numberline {5.2}Multiple Latent Filters}{42}
\contentsline {subsection}{\numberline {5.2.1}Architecture}{42}
\contentsline {subsection}{\numberline {5.2.2}Constructing a Suitable Loss Function}{43}
\contentsline {subsection}{\numberline {5.2.3}Neuron-Level Decoupling}{44}
\contentsline {subsection}{\numberline {5.2.4}Na{\"i}ve Filter-Level Decoupling}{44}
\contentsline {subsection}{\numberline {5.2.5}Weighted Filter-Level Decoupling}{45}
\contentsline {section}{\numberline {5.3}Separating Colour Spaces}{46}
\contentsline {subsection}{\numberline {5.3.1}Architecture}{46}
\contentsline {subsection}{\numberline {5.3.2}Derivation}{46}
\contentsline {section}{\numberline {5.4}Orthogonal Convolutions}{47}
\contentsline {subsection}{\numberline {5.4.1}Architecture}{47}
\contentsline {subsection}{\numberline {5.4.2}Derivation}{47}
\contentsline {section}{\numberline {5.5}Winner Takes All}{47}
\contentsline {subsection}{\numberline {5.5.1}Architecture}{48}
\contentsline {subsection}{\numberline {5.5.2}Derivation}{48}
\contentsline {chapter}{\numberline {6}Results}{49}
\contentsline {section}{\numberline {6.1}The No Free Lunch Relationship Between Reconstruction Loss and KL Divergence}{49}
\contentsline {section}{\numberline {6.2}Using Batch Normalisation With Convolutional Variational Latent Spaces}{49}
\contentsline {section}{\numberline {6.3}Single Latent Filter}{50}
\contentsline {section}{\numberline {6.4}Disentangling Latent Neurons}{50}
\contentsline {section}{\numberline {6.5}Disentangling Latent Filters Using Averages}{50}
\contentsline {section}{\numberline {6.6}Decoupling Latent Filters Using Weighted-Averages}{51}
\contentsline {section}{\numberline {6.7}Separating Colour Spaces}{51}
\contentsline {section}{\numberline {6.8}Orthogonal Convolutions}{51}
\contentsline {section}{\numberline {6.9}Winner Takes All}{52}
\contentsline {chapter}{\numberline {7}Conclusion}{53}
\contentsline {section}{\numberline {7.1}Summary of Thesis Achievements}{53}
\contentsline {section}{\numberline {7.2}Applications}{53}
\contentsline {section}{\numberline {7.3}Future Work}{53}
\contentsline {chapter}{Bibliography}{53}
