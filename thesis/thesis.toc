\contentsline {chapter}{Abstract}{i}
\contentsline {chapter}{Acknowledgements}{iii}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Motivation}{1}
\contentsline {section}{\numberline {1.2}Objectives}{4}
\contentsline {section}{\numberline {1.3}Contributions}{5}
\contentsline {chapter}{\numberline {2}Background}{6}
\contentsline {section}{\numberline {2.1}Loss functions}{6}
\contentsline {subsection}{\numberline {2.1.1}Euclidean Distance}{7}
\contentsline {subsection}{\numberline {2.1.2}Binary Cross-Entropy}{7}
\contentsline {section}{\numberline {2.2}Stella}{8}
\contentsline {section}{\numberline {2.3}Arcade Learning Environment}{8}
\contentsline {section}{\numberline {2.4}Keras}{9}
\contentsline {section}{\numberline {2.5}TensorBoard}{9}
\contentsline {chapter}{\numberline {3}Related Work}{10}
\contentsline {section}{\numberline {3.1}Autoencoders}{10}
\contentsline {subsection}{\numberline {3.1.1}Fully-Connected Autoencoders}{11}
\contentsline {subsection}{\numberline {3.1.2}Fully-Convolutional Autoencoders}{13}
\contentsline {section}{\numberline {3.2}Variational Autoencoders}{15}
\contentsline {subsection}{\numberline {3.2.1}A Probabilistic Perspective}{15}
\contentsline {subsection}{\numberline {3.2.2}Overcoming the Intractable Posterior}{16}
\contentsline {subsection}{\numberline {3.2.3}Finding a Suitable Loss Function: the ELBO}{16}
\contentsline {subsection}{\numberline {3.2.4}Writing the ELBO in Closed-Form}{18}
\contentsline {subsection}{\numberline {3.2.5}Implementing the Variational Autoencoder}{20}
\contentsline {subsection}{\numberline {3.2.6}Intuition Behind the Variational Autoencoder}{22}
\contentsline {section}{\numberline {3.3}Disentangled Representations}{27}
\contentsline {section}{\numberline {3.4}Unsupervised Learning of Generative Factors}{27}
\contentsline {subsection}{\numberline {3.4.1}InfoGAN}{27}
\contentsline {subsection}{\numberline {3.4.2}$\beta $-VAE}{27}
\contentsline {subsubsection}{Derivation}{28}
\contentsline {section}{\numberline {3.5}Improving Sampling from Generative Autoencoders with Markov Chains}{31}
\contentsline {section}{\numberline {3.6}Regularizing CNNs with Locally Constrained Decorrelations}{33}
\contentsline {section}{\numberline {3.7}Batch Normalisation}{33}
\contentsline {chapter}{\numberline {4}Implementation}{34}
\contentsline {section}{\numberline {4.1}Dimensionality Reduction}{34}
\contentsline {subsection}{\numberline {4.1.1}Pre-processing Pipeline}{35}
\contentsline {subsubsection}{Ensuring Object Persistence}{35}
\contentsline {subsubsection}{Extracting Luminance and Cropping}{35}
\contentsline {subsubsection}{File Formats}{35}
\contentsline {section}{\numberline {4.2}Qualitative Assessment Using GUIs}{35}
\contentsline {section}{\numberline {4.3}Training and Validation Data Generators}{37}
\contentsline {section}{\numberline {4.4}Ensuring Numerical Stability in the Latent Space}{37}
\contentsline {section}{\numberline {4.5}Activation Functions in the Latent Space}{38}
\contentsline {section}{\numberline {4.6}Keras Callbacks}{38}
\contentsline {chapter}{\numberline {5}Methods}{39}
\contentsline {section}{\numberline {5.1}Single Latent Filter}{40}
\contentsline {subsection}{\numberline {5.1.1}Architecture}{40}
\contentsline {subsection}{\numberline {5.1.2}Neuron-Level Decoupling}{41}
\contentsline {section}{\numberline {5.2}Multiple Latent Filters}{42}
\contentsline {subsection}{\numberline {5.2.1}Architecture}{42}
\contentsline {subsection}{\numberline {5.2.2}Neuron-Level Decoupling}{43}
\contentsline {subsection}{\numberline {5.2.3}Na{\"i}ve Filter-Level Decoupling}{44}
\contentsline {subsection}{\numberline {5.2.4}Weighted Filter-Level Decoupling}{45}
\contentsline {section}{\numberline {5.3}Separating Colour Spaces}{46}
\contentsline {section}{\numberline {5.4}Winner Takes All}{47}
\contentsline {subsection}{\numberline {5.4.1}Derivation}{47}
\contentsline {section}{\numberline {5.5}Orthogonal Convolutions}{49}
\contentsline {subsection}{\numberline {5.5.1}Architecture}{49}
\contentsline {subsection}{\numberline {5.5.2}Derivation}{49}
\contentsline {chapter}{\numberline {6}Results}{50}
\contentsline {section}{\numberline {6.1}Learning Generative Factors in Dense Latent Spaces}{50}
\contentsline {section}{\numberline {6.2}Single Latent Filter}{50}
\contentsline {section}{\numberline {6.3}Disentangling Latent Neurons}{51}
\contentsline {section}{\numberline {6.4}Disentangling Latent Filters Using Averages}{53}
\contentsline {section}{\numberline {6.5}Decoupling Latent Filters Using Weighted-Averages}{54}
\contentsline {section}{\numberline {6.6}Separating Colour Spaces}{57}
\contentsline {section}{\numberline {6.7}Orthogonal Convolutions}{58}
\contentsline {section}{\numberline {6.8}Winner Takes All}{61}
\contentsline {section}{\numberline {6.9}The No Free Lunch Relationship Between Reconstruction Loss and KL Divergence}{63}
\contentsline {section}{\numberline {6.10}Using Batch Normalisation With Convolutional Variational Latent Spaces}{64}
\contentsline {chapter}{\numberline {7}Conclusion}{65}
\contentsline {section}{\numberline {7.1}Summary of Thesis Achievements}{65}
\contentsline {section}{\numberline {7.2}Applications}{65}
\contentsline {section}{\numberline {7.3}Future Work}{65}
\contentsline {chapter}{Bibliography}{65}
