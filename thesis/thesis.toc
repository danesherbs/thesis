\contentsline {chapter}{Acknowledgements}{i}
\contentsline {chapter}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {1.1}Motivation}{1}
\contentsline {section}{\numberline {1.2}Objectives}{4}
\contentsline {section}{\numberline {1.3}Contributions}{5}
\contentsline {chapter}{\numberline {2}Background}{6}
\contentsline {section}{\numberline {2.1}Loss functions}{6}
\contentsline {subsection}{\numberline {2.1.1}Euclidean Distance}{7}
\contentsline {subsection}{\numberline {2.1.2}Binary Cross-Entropy}{7}
\contentsline {section}{\numberline {2.2}Stella}{8}
\contentsline {section}{\numberline {2.3}Arcade Learning Environment}{8}
\contentsline {section}{\numberline {2.4}Keras}{9}
\contentsline {section}{\numberline {2.5}TensorBoard}{9}
\contentsline {chapter}{\numberline {3}Related Work}{10}
\contentsline {section}{\numberline {3.1}Autoencoders}{10}
\contentsline {subsection}{\numberline {3.1.1}Fully-Connected Autoencoders}{11}
\contentsline {subsection}{\numberline {3.1.2}Fully-Convolutional Autoencoders}{13}
\contentsline {section}{\numberline {3.2}Variational Autoencoders}{15}
\contentsline {subsection}{\numberline {3.2.1}A Probabilistic Perspective}{15}
\contentsline {subsection}{\numberline {3.2.2}Overcoming the Intractable Posterior}{16}
\contentsline {subsection}{\numberline {3.2.3}Finding a Suitable Loss Function: the ELBO}{16}
\contentsline {subsection}{\numberline {3.2.4}Writing the ELBO in Closed-Form}{18}
\contentsline {subsection}{\numberline {3.2.5}Implementing the Variational Autoencoder}{20}
\contentsline {subsection}{\numberline {3.2.6}Intuition Behind the Variational Autoencoder}{22}
\contentsline {section}{\numberline {3.3}Disentangled Representations}{27}
\contentsline {section}{\numberline {3.4}Unsupervised Learning of Generative Factors}{27}
\contentsline {subsection}{\numberline {3.4.1}InfoGAN}{27}
\contentsline {subsection}{\numberline {3.4.2}$\beta $-VAE}{27}
\contentsline {subsubsection}{Derivation}{28}
\contentsline {section}{\numberline {3.5}Improving Sampling from Generative Autoencoders with Markov Chains}{31}
\contentsline {section}{\numberline {3.6}Regularizing CNNs with Locally Constrained Decorrelations}{33}
\contentsline {section}{\numberline {3.7}Batch Normalisation}{33}
\contentsline {chapter}{\numberline {4}Methods}{34}
\contentsline {section}{\numberline {4.1}Dimensionality Reduction}{34}
\contentsline {subsection}{\numberline {4.1.1}Pre-processing Pipeline}{35}
\contentsline {subsubsection}{Ensuring Object Persistence}{35}
\contentsline {subsubsection}{Extracting Luminance and Cropping}{35}
\contentsline {subsubsection}{File Formats}{35}
\contentsline {section}{\numberline {4.2}Qualitative Assessment Using GUIs}{37}
\contentsline {section}{\numberline {4.3}Latent Image}{37}
\contentsline {subsection}{\numberline {4.3.1}Architecture}{38}
\contentsline {subsection}{\numberline {4.3.2}Derivation}{38}
\contentsline {section}{\numberline {4.4}Disentangling Latent Neurons}{38}
\contentsline {subsection}{\numberline {4.4.1}Architecture}{39}
\contentsline {subsection}{\numberline {4.4.2}Derivation}{40}
\contentsline {section}{\numberline {4.5}Disentangling Latent Filters Using Averages}{40}
\contentsline {subsection}{\numberline {4.5.1}Architecture}{40}
\contentsline {subsection}{\numberline {4.5.2}Derivation}{41}
\contentsline {section}{\numberline {4.6}Decoupling Latent Filters Using Weighted-Averages}{41}
\contentsline {subsection}{\numberline {4.6.1}Architecture}{41}
\contentsline {subsection}{\numberline {4.6.2}Derivation}{41}
\contentsline {section}{\numberline {4.7}Separating Colour Spaces}{41}
\contentsline {subsection}{\numberline {4.7.1}Architecture}{42}
\contentsline {subsection}{\numberline {4.7.2}Derivation}{42}
\contentsline {section}{\numberline {4.8}Orthogonal Convolutions}{42}
\contentsline {subsection}{\numberline {4.8.1}Architecture}{42}
\contentsline {subsection}{\numberline {4.8.2}Derivation}{42}
\contentsline {section}{\numberline {4.9}Winner Takes All}{42}
\contentsline {subsection}{\numberline {4.9.1}Architecture}{43}
\contentsline {subsection}{\numberline {4.9.2}Derivation}{43}
\contentsline {chapter}{\numberline {5}Results}{44}
\contentsline {section}{\numberline {5.1}The No Free Lunch Relationship Between Reconstruction Loss and KL Divergence}{44}
\contentsline {section}{\numberline {5.2}Using Batch Normalisation With Convolutional Variational Latent Spaces}{44}
\contentsline {section}{\numberline {5.3}Using Batch Normalisation With Convolutional Variational Latent Spaces}{44}
\contentsline {chapter}{\numberline {6}Conclusion}{46}
\contentsline {section}{\numberline {6.1}Summary of Thesis Achievements}{46}
\contentsline {section}{\numberline {6.2}Applications}{46}
\contentsline {section}{\numberline {6.3}Future Work}{46}
\contentsline {chapter}{Bibliography}{46}
