\chapter{Introduction}

\section{Motivation}

A long term goal of artificial intelligence (AI) is the development of artificial general intelligence (AGI). Since the field's inception in the 1950s, it has swung between hype and breakthroughs, followed by disappointment and reduced funding, known as AI winters \cite{Knight2016}. During the first period of hype from the 50s to the early 70s, Marvin Minsky made the following prediction: \cite{Darrach}

\begin{quote} 
\centering 
``In from three to eight years we will have a machine with the general intelligence of an average human being." - Marvin Minsky, 1970
\end{quote}

This prediction was clearly not realised, and the first AI winter would shortly follow.

Symbolic AI was developed during this winter, which encodes knowledge as human-readable rules and facts, making it easy to comprehend chains of actions and abstract relationships \cite{Reingold2001}. For instance, given the unary relations \texttt{red} and \texttt{strawberry}, and the binary relation \texttt{bigger}, we can say that \texttt{A} is the smallest red strawberry by writing $$\texttt{red(A)} \quad \texttt{strawberry(A)} \quad \forall\texttt{B bigger(B, A)}$$ But given the unary relations \texttt{yellow} and \texttt{banana} we could also write that \texttt{A} is the third biggest yellow strawberry, or a red banana, and so on. We can see that the rules and facts in symbolic logic can be endlessly recombined and extended. This allows for the manipulation of high-level abstract concepts, which is key to AGI \cite{Garnelo2016}.

However, symbolic AI has a major philosophical problem: the facts and rules are only meaningful to the human writing them; their meaning is not intrinsic to the system itself. This is known as the \textit{symbol grounding problem}.

Today we find ourselves in yet another period of hype and exciting breakthroughs. Reinforcement learning (RL) has become a prominent area of research, with many considering it fundemental for AGI \cite{Hutter2005}, as have deep neural networks. Recently, deep reinforcement learning (DRL) systems have achieved impressive feats, including mastering a wide range of Atari 2600 games to a superhuman level using only raw pixels and score as input, and the board game Go \cite{Mnih2015, Silver2016}.\\

\begin{figure}[h!]
\centering
\begin{minipage}{.45\textwidth}
  \centering
\includegraphics[width=\textwidth]{introduction/deep_blue_kasparov.jpg}
  \caption{May 1997: Gary Kasparov makes his first move against IBM's Deep Blue. Deep Blue would later emerge the victor in the best of six games; the first time a reigning world chess champion is defeated by a computer. \cite{Rosen2012}.}
  \label{fig:deep_blue_kasparov}
\end{minipage}%
  \hfill
\begin{minipage}{.45\textwidth}
  \centering
\includegraphics[width=\textwidth]{introduction/alpha_go_first_move_game_three.png}
  \caption{March 2016: Lee Sedol, one of the greatest modern Go players, plays his first move of game three against AlphaGo. AlphaGo won four of five games. This feat was considered by many to be a decade away. \cite{Ormerod2016}.}
  \label{fig:alpha_go_first_move_game_three}
\end{minipage}
\end{figure}

Though DRL systems are not afflicted by the same problems as symbolic AI, they have a number of drawbacks of their own. Namely, they are: \cite{Garnelo2016}

\begin{enumerate}
\item \textbf{Slow to learn}. Neural networks require large data sets and are therefore
slow to learn.
\item \textbf{Unable to transfer past experience}. They often fail to perform well on tasks very
similar to those they have mastered.
\item \textbf{Unable to reason abstractly}. They fail to exploit statistical regularities in the data.
\item \textbf{Hard to reason about}. It's often difficult to extract a comprehensible chain of reasons for why a deep neural network operated in the way it did.
\end{enumerate}

Deep symbolic reinforcement learning (DSRL) is the marrying of DRL and symbolic AI; a recent advance which overcomes the symbol grounding problem and the drawbacks associated with DRL \cite{Garnelo2016}. That is, DSRL systems overcome the symbol grounding problem, and are:

\begin{enumerate}
\item \textbf{Fast to learn}. Large data sets are not necessary.
\item \textbf{Able to transfer past experience}. Symbolic AI lends itself to multiple processes associated with high-level reasoning, including transfer learning.
\item \textbf{Able to reason abstractly}. The agent is able to exploit statistical regularities in the
training data by using high-level processes like planning or causal reasoning.
\item \textbf{Easy to reason about}. Since the front end uses symbolic AI, its knowledge is encoded as human-readable facts and rules, making the extraction of comprehensible chains of logic much easier.
\end{enumerate}

\begin{figure}[h!]
\centering
\captionsetup{justification=centering}
\includegraphics[width=\textwidth]{introduction/DSRL_architecture.eps}
\caption{Overview of deep symbolic reinforcement learning system architecture. \textbf{A}: The neural back end maps high-dimensional raw input data to a compositionally structured symbolic representation. \textbf{B}: The compositionally structured symbolic representation. \textbf{C}: Reinforcement learning of mapping from symbolic representation to action with maximum expected reward over time. \textit{Adapted from: Garnelo et al.} \cite{Garnelo2016}.}
\label{fig:dsrl_archiecture}
\end{figure}

The DSRL framework is shown in Figure (\ref{fig:dsrl_archiecture}). The neural back end takes a high-dimensional input and outputs a symbolic representation. This symbolic representation is then fed to the symbolic front end, whose role is action selection. The agent then acts on the environment and obtains a reward and the sensory input of the next time step. As the neural back end learns how to represent the raw input data in a compositionally structured representation in an unsupervised manner, and the symbolic front end learns to select the action with maximum expected reward over time, the system as a whole learns end-to-end.

The unsupervised extraction of features from a wide range of scenes is still a challenge in AI research. Since DSRL systems require this extraction in forming the symbolic representation, the current system only works for toy data sets. In order for the system to scale to complex scenes, a scalable method of extracting an object's type and position must be found.

\section{Contributions}

With a recent development, $\beta$-VAE, it is possible to learn independent generative factors in complex scenes using variational autoencoders \cite{Higgins2016}. In this thesis, we apply the technique proposed in $\beta$-VAE to the novel fully-convolutional variational autoencoder architecture, and in turn assess the feasibility of this architecture for advancing DSRL. We approach this in the following way:
\begin{itemize}
\item \textbf{Propose the novel architecture of the fully-convolutional variational autoencoder.} This is (as far as we know) the first consideration of a fully-convolutional variational autoencoder.
\item \textbf{Propose a number of novel methods to solve the low-level extraction of objects and their location in fully-convolutional variational autoencoders.} This is done by inventing novel architectures and loss functions, which are inspired by the developments made in $\beta$-VAE. These approaches are shown to be good starting points in solving this open problem.
\item \textbf{Collect experimental evidence for each proposed method.} Each proposed method is assessed by a range of experiments. These include reconstructing its input, qualitatively examining the latent representation of high-level objects in the scene as well as generating new samples by sampling from the prior and using Markov chain Monte Carlo (MCMC).
\item \textbf{Assess the feasibility of using fully-convolutional variational autoencoders to advance DSRL.} Upon assessment of the experiments, we propose a suitable method that (at least) partially solves the problem of latent representation of high-level objects in the scene.
\end{itemize}